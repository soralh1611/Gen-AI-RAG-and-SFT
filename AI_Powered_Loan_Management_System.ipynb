{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPzzOW9uFp3juez2+rbwFid",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soralh1611/Gen-AI-RAG-and-SFT/blob/main/AI_Powered_Loan_Management_System.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Overview**\n",
        "\n",
        "# Project\n",
        "\n",
        "Soral: Intelligent Underwriting Assistant\n",
        "SFT-Tuned Gemini with RAG & State-Machine Orchestration\n",
        "1. Executive Summary\n",
        "Artha Underwriter 2.0 is an advanced AI agent designed to automate the initial stages of loan eligibility assessment. By combining Supervised Fine-Tuning (SFT) for persona adherence with Retrieval-Augmented Generation (RAG) for factual grounding, the system provides accurate, policy-compliant underwriting decisions in a playful yet professional conversational format.\n",
        "\n",
        "2. Technical Architecture\n",
        "The system is built upon three primary pillars of modern LLM engineering:\n",
        "\n",
        "Supervised Fine-Tuning (SFT): The base Gemini 2.0 model was fine-tuned on a specialized dataset of underwriting scenarios to master the \"Soral\" persona and internalize complex financial logic.\n",
        "\n",
        "Retrieval-Augmented Generation (RAG): Real-time integration with institutional policy documents ensures that every decision is anchored in current lending standards, significantly mitigating hallucinations.\n",
        "\n",
        "LLM Orchestration: A custom state-machine layer manages the conversation flow, ensuring a strict, non-looping 4-step data collection process (Name ‚Üí ID ‚Üí FICO ‚Üí Income).\n",
        "\n",
        "3. Core Features\n",
        "Linear Data Gates: Validates user inputs (e.g., 6-digit ID check) and prevents the model from proceeding until current requirements are met.\n",
        "\n",
        "Anti-Loop Safeguards: Utilizes custom Frequency and Presence Penalties to maintain conversational momentum and prevent repetitive questioning.\n",
        "\n",
        "Safety & Compliance: Implemented rigorous safety filters to block derogatory language and identify fraudulent input patterns.\n",
        "\n",
        "4. Notebook Structure\n",
        "Environment Setup: Library installations and Vertex AI initialization.\n",
        "\n",
        "Model Configuration: Loading the SFT-tuned model from the Vertex AI Registry.\n",
        "\n",
        "RAG Integration: Connecting to the knowledge base for grounded decision-making.\n",
        "\n",
        "Orchestration Logic: Defining the state-machine and generation parameters.\n",
        "\n",
        "Gradio Interface: Launching the interactive underwriting chatbot."
      ],
      "metadata": {
        "id": "26Mq0gRlPiZ9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8ytArPkgWy4",
        "outputId": "7e88f2c1-3e76-4157-b0b3-b9caff690ad1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vertex AI SDK initialized for project soral-vertex-a\n"
          ]
        }
      ],
      "source": [
        "from google.cloud import aiplatform\n",
        "\n",
        "# Define your project variables\n",
        "PROJECT_ID = \"soral-vertex-a\"\n",
        "REGION = \"us-central1\"\n",
        "BUCKET_URI = \"gs://soral-lms_bucket\" # Used for storing model artifacts\n",
        "\n",
        "# Initialize the Vertex AI SDK\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)\n",
        "\n",
        "print(f\"Vertex AI SDK initialized for project {PROJECT_ID}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "import vertexai\n",
        "vertexai.init(project=\"soral-vertex-a\", location=\"us-central1\")"
      ],
      "metadata": {
        "id": "R3wUw-ycuxY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import storage\n",
        "\n",
        "def create_new_bucket(bucket_name, location=\"us-central1\"):\n",
        "    \"\"\"Creates a new bucket in the specified location.\"\"\"\n",
        "    storage_client = storage.Client(project=PROJECT_ID)\n",
        "\n",
        "    # 1. Clean name (remove gs:// if accidentally added)\n",
        "    clean_name = bucket_name.replace(\"gs://\", \"\").lower()\n",
        "\n",
        "    try:\n",
        "        # 2. Check if it already exists\n",
        "        if storage_client.lookup_bucket(clean_name):\n",
        "            print(f\"‚ö†Ô∏è Bucket '{clean_name}' already exists.\")\n",
        "            return storage_client.get_bucket(clean_name)\n",
        "\n",
        "        # 3. Create the bucket\n",
        "        bucket = storage_client.create_bucket(clean_name, location=location)\n",
        "\n",
        "        # 4. Optional: Enable Uniform Bucket-Level Access (Recommended for AI projects)\n",
        "        bucket.iam_configuration.uniform_bucket_level_access_enabled = True\n",
        "        bucket.patch()\n",
        "\n",
        "        print(f\"‚úÖ Success: Bucket '{bucket.name}' created in {location}\")\n",
        "        return bucket\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error creating bucket: {e}\")\n",
        "\n",
        "# Call the function with a unique name\n",
        "# Tip: Use your name or project ID as a prefix\n",
        "NEW_BUCKET_NAME = \"lms-reports-soral-2025\"\n",
        "my_bucket = create_new_bucket(NEW_BUCKET_NAME)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84PcY1aozv7j",
        "outputId": "b98e78fc-ced7-448f-d1b2-d965d88011c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ö†Ô∏è Bucket 'lms-reports-soral-2025' already exists.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install faker reportlab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkQjmMJ5PMo9",
        "outputId": "e6c77291-3a6e-4f59-988b-24434620ceab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faker\n",
            "  Downloading faker-39.0.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting reportlab\n",
            "  Downloading reportlab-4.4.7-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: tzdata in /usr/local/lib/python3.12/dist-packages (from faker) (2025.3)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.12/dist-packages (from reportlab) (11.3.0)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.12/dist-packages (from reportlab) (3.4.4)\n",
            "Downloading faker-39.0.0-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading reportlab-4.4.7-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m69.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: reportlab, faker\n",
            "Successfully installed faker-39.0.0 reportlab-4.4.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from faker import Faker\n",
        "from reportlab.platypus import SimpleDocTemplate, Table, TableStyle, Paragraph\n",
        "from reportlab.lib.styles import getSampleStyleSheet\n",
        "from reportlab.lib import colors\n",
        "import random\n",
        "\n",
        "fake = Faker()\n",
        "\n",
        "def generate_bank_statement(filename, account_holder):\n",
        "    doc = SimpleDocTemplate(filename)\n",
        "    elements = []\n",
        "    styles = getSampleStyleSheet()\n",
        "\n",
        "    # Header\n",
        "    elements.append(Paragraph(f\"<b>Bank of Vertex AI - Monthly Statement</b>\", styles['Title']))\n",
        "    elements.append(Paragraph(f\"Account Holder: {account_holder}\", styles['Normal']))\n",
        "    elements.append(Paragraph(f\"Statement Period: Dec 2025\", styles['Normal']))\n",
        "\n",
        "    # Transaction Data\n",
        "    data = [[\"Date\", \"Description\", \"Amount\", \"Balance\"]]\n",
        "    balance = 5000.00\n",
        "    for _ in range(15):\n",
        "        date = f\"2025-12-{random.randint(1, 20):02d}\"\n",
        "        desc = fake.company()\n",
        "        amt = round(random.uniform(-500, 1000), 2)\n",
        "        balance += amt\n",
        "        data.append([date, desc, f\"${amt}\", f\"${round(balance, 2)}\"])\n",
        "\n",
        "    # Table Styling\n",
        "    t = Table(data)\n",
        "    t.setStyle(TableStyle([\n",
        "        ('BACKGROUND', (0, 0), (-1, 0), colors.grey),\n",
        "        ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),\n",
        "        ('GRID', (0, 0), (-1, -1), 1, colors.black)\n",
        "    ]))\n",
        "    elements.append(t)\n",
        "    doc.build(elements)\n",
        "\n",
        "generate_bank_statement(\"bank_statement_demo.pdf\", \"John Doe\")"
      ],
      "metadata": {
        "id": "ROmGNeH0QgvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install faker faker-credit-score reportlab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AaOZnLyYSUas",
        "outputId": "f0a879ea-0a6d-41b6-e2cf-2aa5a159ec89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faker in /usr/local/lib/python3.12/dist-packages (39.0.0)\n",
            "Collecting faker-credit-score\n",
            "  Downloading faker_credit_score-0.5.2-py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: reportlab in /usr/local/lib/python3.12/dist-packages (4.4.7)\n",
            "Requirement already satisfied: tzdata in /usr/local/lib/python3.12/dist-packages (from faker) (2025.3)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.12/dist-packages (from reportlab) (11.3.0)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.12/dist-packages (from reportlab) (3.4.4)\n",
            "Downloading faker_credit_score-0.5.2-py3-none-any.whl (5.8 kB)\n",
            "Installing collected packages: faker-credit-score\n",
            "Successfully installed faker-credit-score-0.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from reportlab.lib.pagesizes import LETTER\n",
        "from reportlab.platypus import SimpleDocTemplate, Table, TableStyle, Paragraph, Spacer\n",
        "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
        "from reportlab.lib import colors\n",
        "from faker_credit_score import CreditScore\n",
        "from faker.providers import DynamicProvider\n",
        "from faker.providers import BaseProvider\n",
        "\n",
        "fake = Faker()\n",
        "fake.add_provider(CreditScore)\n",
        "\n",
        "def generate_credit_report(filename, applicant_name):\n",
        "    doc = SimpleDocTemplate(filename, pagesize=LETTER)\n",
        "    styles = getSampleStyleSheet()\n",
        "    elements = []\n",
        "\n",
        "    # Custom Style for \"Confidential\" Header\n",
        "    header_style = ParagraphStyle('HeaderStyle', parent=styles['Normal'], fontSize=10, textColor=colors.red)\n",
        "\n",
        "    # 1. Header Section\n",
        "    elements.append(Paragraph(\"EQUIFAX - CONFIDENTIAL CONSUMER CREDIT FILE\", header_style))\n",
        "    elements.append(Spacer(1, 12))\n",
        "    elements.append(Paragraph(f\"<b>Subject:</b> {applicant_name}\", styles['Title']))\n",
        "    elements.append(Paragraph(f\"<b>File Number:</b> {fake.uuid4()}\", styles['Normal']))\n",
        "    elements.append(Paragraph(f\"<b>Date of Report:</b> Dec 21, 2025\", styles['Normal']))\n",
        "    elements.append(Spacer(1, 20))\n",
        "\n",
        "    # 2. Credit Score Section (The \"Big Number\")\n",
        "    score = fake.credit_score()\n",
        "    score_data = [[f\"EQUIFAX BEACON 5.0 SCORE: {score}\"]]\n",
        "    score_table = Table(score_data, colWidths=[400])\n",
        "    score_table.setStyle(TableStyle([\n",
        "        ('BACKGROUND', (0, 0), (-1, -1), colors.lightgrey),\n",
        "        ('ALIGN', (0, 0), (-1, -1), 'CENTER'),\n",
        "        ('FONTSIZE', (0, 0), (-1, -1), 18),\n",
        "        ('BOTTOMPADDING', (0, 0), (-1, -1), 10),\n",
        "    ]))\n",
        "    elements.append(score_table)\n",
        "    elements.append(Spacer(1, 20))\n",
        "\n",
        "    # 3. Trade Lines (Credit Accounts)\n",
        "    elements.append(Paragraph(\"<b>ACCOUNT HISTORY (TRADE LINES)</b>\", styles['Heading2']))\n",
        "    trade_data = [[\"Creditor\", \"Account Type\", \"Balance\", \"Status\"]]\n",
        "\n",
        "    # 1. DEFINE the class first\n",
        "    class BankProvider(BaseProvider):\n",
        "      def bank_name(self):\n",
        "          banks = [\n",
        "              \"Chase Bank\", \"Wells Fargo\", \"Bank of America\",\n",
        "              \"Vertex AI Financial\", \"Gemini Trust\", \"Goldman Sachs\",\n",
        "              \"PNC Bank\", \"Citigroup\", \"Barclays\"\n",
        "          ]\n",
        "          return self.random_element(banks)\n",
        "    # 4. Add your custom provider to the Faker instance\n",
        "    fake.add_provider(BankProvider)\n",
        "    for _ in range(5):\n",
        "        trade_data.append([\n",
        "            fake.bank_name(),\n",
        "            random.choice([\"Revolving\", \"Installment\", \"Mortgage\"]),\n",
        "            f\"${fake.random_int(0, 15000)}\",\n",
        "            random.choice([\"Current\", \"30 Days Past Due\", \"Paid as Agreed\"])\n",
        "        ])\n",
        "\n",
        "    t = Table(trade_data, colWidths=[150, 100, 80, 120])\n",
        "    t.setStyle(TableStyle([\n",
        "        ('GRID', (0, 0), (-1, -1), 0.5, colors.grey),\n",
        "        ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),\n",
        "        ('BACKGROUND', (0, 0), (-1, 0), colors.whitesmoke)\n",
        "    ]))\n",
        "    elements.append(t)\n",
        "\n",
        "    doc.build(elements)\n",
        "\n",
        "    from google.cloud import storage\n",
        "\n",
        "\n",
        "def upload_to_gcs(bucket_name, source_file_name, destination_blob_name):\n",
        "    storage_client = storage.Client()\n",
        "    clean_name = bucket_name.replace(\"gs://\", \"\")\n",
        "    bucket = storage_client.get_bucket(clean_name)\n",
        "    blob = bucket.blob(destination_blob_name)\n",
        "    blob.upload_from_filename(source_file_name)\n",
        "    print(f\"‚úÖ Success: Uploaded {source_file_name} to {clean_name}\")\n",
        "\n",
        "# --- EXECUTION STEPS ---\n",
        "\n",
        "# Set your names\n",
        "MY_BUCKET = \"lms-reports-soral-2025\"\n",
        "FILE_NAME = \"synthetic_report.pdf\"\n",
        "\n",
        "# STEP 1: Generate the file (Fixes Errno 2)\n",
        "generate_credit_report(FILE_NAME, \"Alex Rivera\")\n",
        "\n",
        "# STEP 2: Now that the file exists, upload it (Fixes 404)\n",
        "upload_to_gcs(MY_BUCKET, FILE_NAME, \"reports/december_report_01.pdf\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbNJpvVGVWEP",
        "outputId": "d446d299-a57e-4a8e-fd64-2e582af4a4d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Success: Uploaded synthetic_report.pdf to lms-reports-soral-2025\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "from faker import Faker\n",
        "from faker.providers import BaseProvider\n",
        "from faker_credit_score import CreditScore\n",
        "from google.cloud import storage\n",
        "from google.cloud.storage import transfer_manager\n",
        "from reportlab.platypus import SimpleDocTemplate, Table, TableStyle, Paragraph, Spacer\n",
        "from reportlab.lib.styles import getSampleStyleSheet\n",
        "from reportlab.lib import colors\n",
        "\n",
        "# 1. SETUP\n",
        "fake = Faker()\n",
        "class BankProvider(BaseProvider):\n",
        "    def bank_name(self):\n",
        "        return self.random_element([\"Chase\", \"Wells Fargo\", \"Vertex AI Bank\", \"Gemini Trust\"])\n",
        "\n",
        "fake.add_provider(BankProvider)\n",
        "fake.add_provider(CreditScore)\n",
        "\n",
        "BUCKET_NAME = \"lms-reports-soral-2025\"\n",
        "LOCAL_DIR = \"bulk_data_reports\"\n",
        "os.makedirs(LOCAL_DIR, exist_ok=True)\n",
        "styles = getSampleStyleSheet()\n",
        "\n",
        "# 2. DATA GENERATION FUNCTION\n",
        "def generate_full_report(i):\n",
        "    name = fake.name()\n",
        "    u_id = f\"{i:04d}\"\n",
        "    filename = os.path.join(LOCAL_DIR, f\"report_{u_id}.pdf\")\n",
        "\n",
        "    doc = SimpleDocTemplate(filename)\n",
        "\n",
        "    # --- CRITICAL: Create a NEW story list for every file ---\n",
        "    story = []\n",
        "\n",
        "    # Add Title\n",
        "    story.append(Paragraph(f\"<b>Financial Audit: {name}</b>\", styles['Title']))\n",
        "    story.append(Spacer(1, 12))\n",
        "\n",
        "    # Add Financial Summary\n",
        "    summary_data = [\n",
        "        [\"Metric\", \"Value\"],\n",
        "        [\"Credit Score\", str(fake.credit_score())],\n",
        "        [\"Monthly Income\", f\"${random.randint(3000, 12000)}\"],\n",
        "        [\"Primary Bank\", fake.bank_name()]\n",
        "    ]\n",
        "    summary_table = Table(summary_data, colWidths=[150, 150])\n",
        "    summary_table.setStyle(TableStyle([('BACKGROUND', (0,0), (-1,0), colors.lightgrey), ('GRID', (0,0), (-1,-1), 1, colors.black)]))\n",
        "    story.append(summary_table)\n",
        "    story.append(Spacer(1, 20))\n",
        "\n",
        "    # Add 15 Mock Transactions (Ensures file is NOT empty)\n",
        "    trans_data = [[\"Date\", \"Merchant\", \"Amount\", \"Type\"]]\n",
        "    for _ in range(15):\n",
        "        trans_data.append([\n",
        "            str(fake.date_this_year()),\n",
        "            fake.company(),\n",
        "            f\"${random.randint(-1000, 2000)}\",\n",
        "            random.choice([\"Debit\", \"Credit\", \"ACH\"])\n",
        "        ])\n",
        "\n",
        "    trans_table = Table(trans_data, colWidths=[80, 150, 80, 80])\n",
        "    trans_table.setStyle(TableStyle([('GRID', (0,0), (-1,-1), 0.5, colors.grey), ('FONTSIZE', (0,0), (-1,-1), 8)]))\n",
        "    story.append(trans_table)\n",
        "\n",
        "    # FINAL STEP: Build PDF\n",
        "    doc.build(story)\n",
        "    return filename\n",
        "\n",
        "# 3. RUN & UPLOAD\n",
        "def run_bulk_and_upload(count=1000):\n",
        "    all_filenames = []\n",
        "    print(f\"üõ†Ô∏è Generating {count} data-rich reports...\")\n",
        "    for i in range(count):\n",
        "        all_filenames.append(os.path.basename(generate_full_report(i)))\n",
        "        if i % 100 == 0: print(f\"Progress: {i}/{count}\")\n",
        "\n",
        "    print(\"üöÄ Bulk Uploading to GCS...\")\n",
        "    client = storage.Client()\n",
        "    bucket = client.bucket(BUCKET_NAME)\n",
        "\n",
        "    transfer_manager.upload_many_from_filenames(\n",
        "        bucket,\n",
        "        all_filenames,\n",
        "        source_directory=LOCAL_DIR,\n",
        "        max_workers=8\n",
        "    )\n",
        "    print(\"‚úÖ All 1,000 files uploaded with data.\")\n",
        "\n",
        "run_bulk_and_upload(1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VM98NgCw8iBj",
        "outputId": "999e0df9-804d-43cb-b61f-ef68d6509197"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üõ†Ô∏è Generating 1000 data-rich reports...\n",
            "Progress: 0/1000\n",
            "Progress: 100/1000\n",
            "Progress: 200/1000\n",
            "Progress: 300/1000\n",
            "Progress: 400/1000\n",
            "Progress: 500/1000\n",
            "Progress: 600/1000\n",
            "Progress: 700/1000\n",
            "Progress: 800/1000\n",
            "Progress: 900/1000\n",
            "üöÄ Bulk Uploading to GCS...\n",
            "‚úÖ All 1,000 files uploaded with data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install streamlit google-genai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYMWig_uZrJE",
        "outputId": "4a99be4d-9abc-4aab-e719-394674795cd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.52.2-py3-none-any.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: google-genai in /usr/local/lib/python3.12/dist-packages (1.55.0)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.2.4)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.3.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<13,>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.32.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (9.1.2)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (4.15.0)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit) (3.1.45)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.5.1)\n",
            "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (4.12.0)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from google-auth[requests]<3.0.0,>=2.14.1->google-genai) (2.43.0)\n",
            "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from google-genai) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.9.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (2.12.3)\n",
            "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (15.0.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (1.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from google-genai) (1.3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (4.25.1)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (2.13.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai) (3.11)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-auth[requests]<3.0.0,>=2.14.1->google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-auth[requests]<3.0.0,>=2.14.1->google-genai) (4.9.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai) (0.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.9.0->google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.9.0->google-genai) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.9.0->google-genai) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (3.0.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (0.30.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-auth[requests]<3.0.0,>=2.14.1->google-genai) (0.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Downloading streamlit-1.52.2-py3-none-any.whl (9.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m60.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 streamlit-1.52.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q streamlit\n",
        "!npm install -g localtunnel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TsPJl6BeZ4ep",
        "outputId": "8f938925-6137-41ce-d1e7-63f2c12264ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1G\u001b[0K‚†ô\u001b[1G\u001b[0K‚†π\u001b[1G\u001b[0K‚†∏\u001b[1G\u001b[0K‚†º\u001b[1G\u001b[0K‚†¥\u001b[1G\u001b[0K‚†¶\u001b[1G\u001b[0K‚†ß\u001b[1G\u001b[0K‚†á\u001b[1G\u001b[0K‚†è\u001b[1G\u001b[0K‚†ã\u001b[1G\u001b[0K‚†ô\u001b[1G\u001b[0K‚†π\u001b[1G\u001b[0K‚†∏\u001b[1G\u001b[0K‚†º\u001b[1G\u001b[0K‚†¥\u001b[1G\u001b[0K‚†¶\u001b[1G\u001b[0K‚†ß\u001b[1G\u001b[0K‚†á\u001b[1G\u001b[0K‚†è\u001b[1G\u001b[0K‚†ã\u001b[1G\u001b[0K‚†ô\u001b[1G\u001b[0K‚†π\u001b[1G\u001b[0K‚†∏\u001b[1G\u001b[0K‚†º\u001b[1G\u001b[0K‚†¥\u001b[1G\u001b[0K\n",
            "added 22 packages in 3s\n",
            "\u001b[1G\u001b[0K‚†¥\u001b[1G\u001b[0K\n",
            "\u001b[1G\u001b[0K‚†¥\u001b[1G\u001b[0K3 packages are looking for funding\n",
            "\u001b[1G\u001b[0K‚†¥\u001b[1G\u001b[0K  run `npm fund` for details\n",
            "\u001b[1G\u001b[0K‚†¥\u001b[1G\u001b[0K\u001b[1mnpm\u001b[22m \u001b[96mnotice\u001b[39m\n",
            "\u001b[1mnpm\u001b[22m \u001b[96mnotice\u001b[39m New \u001b[31mmajor\u001b[39m version of npm available! \u001b[31m10.8.2\u001b[39m -> \u001b[34m11.7.0\u001b[39m\n",
            "\u001b[1mnpm\u001b[22m \u001b[96mnotice\u001b[39m Changelog: \u001b[34mhttps://github.com/npm/cli/releases/tag/v11.7.0\u001b[39m\n",
            "\u001b[1mnpm\u001b[22m \u001b[96mnotice\u001b[39m To update run: \u001b[4mnpm install -g npm@11.7.0\u001b[24m\n",
            "\u001b[1mnpm\u001b[22m \u001b[96mnotice\u001b[39m\n",
            "\u001b[1G\u001b[0K‚†¥\u001b[1G\u001b[0K"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q gradio google-genai"
      ],
      "metadata": {
        "id": "-fskGINLU7to"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "from google.cloud import storage\n",
        "\n",
        "# --- 1. CONFIG ---\n",
        "BUCKET_NAME = \"lms-reports-soral-2025\"\n",
        "FOLDER_NAME = \"credit-reports\"\n",
        "\n",
        "# 2. GENERATE DATA\n",
        "reports = []\n",
        "# 10 PASSING (IDs 101-110)\n",
        "for i in range(101, 111):\n",
        "    reports.append({\n",
        "        \"id\": str(i), \"name\": f\"Qualified Customer {i}\", \"fico\": 750,\n",
        "        \"annual_income\": 100000, \"requested_loan\": 20000, # LTI 20%\n",
        "        \"inquiries_120d\": 0, \"foreclosures_24m\": 0, \"new_trades_24m\": 0, \"cc_utilization\": 30\n",
        "    })\n",
        "\n",
        "# 10 FAILING (IDs 901-910) - Each fails one specific rule\n",
        "scenarios = [\n",
        "    {\"id\": \"901\", \"requested_loan\": 90000, \"annual_income\": 100000}, # Fail LTI (90%)\n",
        "    {\"id\": \"902\", \"inquiries_120d\": 2},                              # Fail Inquiries\n",
        "    {\"id\": \"903\", \"foreclosures_24m\": 1},                            # Fail Foreclosure\n",
        "    {\"id\": \"904\", \"new_trades_24m\": 1},                               # Fail New Trade\n",
        "    {\"id\": \"905\", \"cc_utilization\": 95},                             # Fail Utilization\n",
        "    {\"id\": \"906\", \"fico\": 620},                                      # Fail Base FICO\n",
        "    {\"id\": \"907\", \"inquiries_120d\": 1, \"cc_utilization\": 85},        # Fail Multiple\n",
        "    {\"id\": \"908\", \"requested_loan\": 60000, \"annual_income\": 70000}, # Fail LTI (85%)\n",
        "    {\"id\": \"909\", \"new_trades_24m\": 2},                               # Fail New Trade\n",
        "    {\"id\": \"910\", \"foreclosures_24m\": 1, \"fico\": 800}                # Fail Foreclosure\n",
        "]\n",
        "reports.extend(scenarios)\n",
        "\n",
        "# 3. UPLOAD TO GCS\n",
        "storage_client = storage.Client()\n",
        "bucket = storage_client.bucket(BUCKET_NAME)\n",
        "\n",
        "for r in reports:\n",
        "    filename = f\"{r['id']}.json\"\n",
        "    blob = bucket.blob(f\"{FOLDER_NAME}/{filename}\")\n",
        "    blob.upload_from_string(json.dumps(r, indent=2), content_type='application/json')\n",
        "\n",
        "print(f\"‚úÖ Successfully uploaded 20 reports to gs://{BUCKET_NAME}/{FOLDER_NAME}/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3h9467YQaYZl",
        "outputId": "0bb878f6-4bab-4f29-8aa0-cecc7a2c3683"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Successfully uploaded 20 reports to gs://lms-reports-soral-2025/credit-reports/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import random\n",
        "\n",
        "# CONFIGURATION FROM POLICY\n",
        "prohibited_states = [\"AR\", \"CO\", \"CT\", \"GA\", \"HI\", \"IA\", \"LA\", \"ME\", \"MA\", \"MS\", \"MT\", \"NH\", \"OR\", \"VT\", \"WV\"]\n",
        "special_apr_states = {\"NY\": \"24.9%\", \"DC\": \"24.9%\", \"PA\": \"24.9%\", \"MD\": \"24.9%\"}\n",
        "\n",
        "# ADVERSE ACTION REASONS (PER POLICY SECTION 6)\n",
        "denial_reasons = {\n",
        "    \"fico\": \"Credit score below minimum requirement of 650.\",\n",
        "    \"income\": \"Income insufficient for amount of credit requested (Minimum $3,000 gross).\",\n",
        "    \"inquiries\": \"Excessive personal loan inquiries in the last 120 days.\",\n",
        "    \"utilization\": \"Excessive revolving credit utilization (Must be <80%).\",\n",
        "    \"id_format\": \"Invalid ID format. Artha Customer IDs must be 1-4 digits only.\"\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "# CONFIGURATION FROM ARTHA POLICY\n",
        "prohibited_states = [\"AR\", \"CO\", \"CT\", \"GA\", \"HI\", \"IA\", \"LA\", \"ME\", \"MA\", \"MS\", \"MT\", \"NH\", \"OR\", \"VT\", \"WV\"]\n",
        "denial_reasons = {\n",
        "    \"fico\": \"Credit score below minimum requirement of 650.\",\n",
        "    \"income\": \"Income insufficient for amount of credit requested (Minimum $3,000 gross).\",\n",
        "    \"inquiries\": \"Excessive personal loan inquiries in the last 120 days.\",\n",
        "    \"utilization\": \"Excessive revolving credit utilization (Must be <80%).\",\n",
        "    \"foreclosure\": \"Foreclosure or repossession on record within the last 24 months.\",\n",
        "    \"bankruptcy\": \"Bankruptcy filing within the last 7 years.\"\n",
        "}\n",
        "\n",
        "dataset = []\n",
        "\n",
        "# 1. ID VALIDATION LOOP (REJECTS 5+ DIGITS)\n",
        "for _ in range(20):\n",
        "    valid_id = str(random.randint(1, 9999))\n",
        "    dataset.append({\"user\": f\"My ID is {valid_id}.\", \"model\": \"ID received. One step at a time: what is your gross monthly income?\"})\n",
        "\n",
        "    invalid_id = str(random.randint(10000, 99999))\n",
        "    dataset.append({\"user\": f\"My ID is {invalid_id}.\", \"model\": \"Invalid ID format. Artha Customer IDs must be between 1 and 4 digits. Please re-enter a valid ID.\"})\n",
        "\n",
        "# 2. ADVERSE ACTION REASONING LOOP (SPECIFIC COMPLIANCE)\n",
        "for gate, reason in denial_reasons.items():\n",
        "    for _ in range(5):\n",
        "        dataset.append({\"user\": f\"Why was my application declined based on my {gate}?\",\n",
        "                        \"model\": f\"[Reasoning: FCRA/ECOA Compliance] Under federal guidelines, we must provide a specific reason: {reason}\"})\n",
        "\n",
        "# 3. SINGLE-QUESTION INTERACTION LOOP (PREVENTS MULTI-INPUT CONFUSION)\n",
        "for _ in range(15):\n",
        "    dataset.append({\"user\": \"I want a loan. My ID is 505 and I live in Texas.\",\n",
        "                    \"model\": \"I see you're in Texas! To ensure accuracy, I only process one item at a time. Let's start with your ID: I've noted ID 505. Now, what is your gross monthly income?\"})\n",
        "\n",
        "# 4. SAFETY & COMMUNITY GUARDRAIL LOOP\n",
        "safety_triggers = [\"This is a scam\", \"I hate this bank\", \"You guys are terrible\", \"[Derogatory Language]\"]\n",
        "for trigger in safety_triggers:\n",
        "    dataset.append({\"user\": trigger, \"model\": \"I am here to assist with professional financial services only. Please keep our conversation respectful and focused on your application.\"})\n",
        "\n",
        "\n",
        "# LOOP 2: MULTI-INPUT HANDLING (SINGLE QUESTION LOGIC)\n",
        "for _ in range(15):\n",
        "    dataset.append({\"user\": \"I want a loan. My ID is 505 and I make $5,000.\",\n",
        "                    \"model\": \"I've noted your ID. To ensure accuracy, I only process one item at a time. First, let's confirm: which state do you live in?\"})\n",
        "\n",
        "# LOOP 3: ADVERSE ACTION REASONS (POLICY GROUNDED)\n",
        "for field, reason in denial_reasons.items():\n",
        "    for _ in range(5): # Generate 5 variations for each denial type\n",
        "        dataset.append({\"user\": f\"Why was my application for {field} denied?\",\n",
        "                        \"model\": f\"[Reasoning: Adverse Action Notice] Your application was declined for the following reason: {reason}\"})\n",
        "\n",
        "# LOOP 4: STATE COMPLIANCE & SPECIAL CAPS\n",
        "for state in prohibited_states[:10]: # Subset for variety\n",
        "    dataset.append({\"user\": f\"I live in {state}. Can I get a loan?\",\n",
        "                    \"model\": f\"No. Artha does not lend in {state}. Visit https://partners.arthafin.com for our partners.\"})\n",
        "# Add to your existing Python loop:\n",
        "loan_products = [\"personal_loan\", \"debt_consolidation\"]\n",
        "other_products = [\"mortgage\", \"auto_loan\", \"solar_loan\"]\n",
        "\n",
        "for _ in range(20):\n",
        "    prod = random.choice(other_products)\n",
        "    dataset.append({\n",
        "        \"user\": f\"I want a {prod} for $20k.\",\n",
        "        \"model\": f\"{{\\\"product_type\\\": \\\"{prod}\\\", \\\"decision\\\": \\\"redirect\\\", \\\"message\\\": \\\"Artha only offers Personal/Debt Consolidation. Redirecting to partners.\\\"}}\"\n",
        "    })\n",
        "\n",
        "# EXPORT TO JSONL\n",
        "with open('artha_fine_tuning_2.jsonl', 'w') as f:\n",
        "    for entry in dataset:\n",
        "        # Standard Gemini Chat Format\n",
        "        json_line = {\n",
        "            \"contents\": [\n",
        "                {\"role\": \"user\", \"parts\": [{\"text\": entry[\"user\"]}]},\n",
        "                {\"role\": \"model\", \"parts\": [{\"text\": entry[\"model\"]}]}\n",
        "            ]\n",
        "        }\n",
        "        f.write(json.dumps(json_line) + '\\n')\n",
        "\n",
        "print(f\"‚úÖ Generated {len(dataset)} lines of tight fintech training data.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-5CebUugjlA",
        "outputId": "27d13455-36e3-40b1-cb44-ad1766104664"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Generated 164 lines of tight fintech training data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import vertexai\n",
        "from vertexai.generative_models import GenerativeModel, Tool\n",
        "# This is the correct path for Grounding and Retrieval classes\n",
        "from vertexai.generative_models import grounding"
      ],
      "metadata": {
        "id": "1yitpdzRWmll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user() # Required to access private dedicated endpoints"
      ],
      "metadata": {
        "id": "CupRlcQKZWh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "{\"contents\": [{\"role\": \"user\", \"parts\": [{\"text\": \"I have a FICO score of 620 and my monthly income is $4500. Am I eligible?\"}]}, {\"role\": \"model\", \"parts\": [{\"text\": \"Underwriting Decision: REJECTED. Reason: FICO score of 620 is below our minimum requirement of 650.\"}]}]}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sgp5vtx0tuiG",
        "outputId": "bc591104-89ba-4139-b914-75213d797d8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'contents': [{'role': 'user',\n",
              "   'parts': [{'text': 'I have a FICO score of 620 and my monthly income is $4500. Am I eligible?'}]},\n",
              "  {'role': 'model',\n",
              "   'parts': [{'text': 'Underwriting Decision: REJECTED. Reason: FICO score of 620 is below our minimum requirement of 650.'}]}]}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# List of your training cases\n",
        "train_cases = [\n",
        "    {\"contents\": [{\"role\": \"user\", \"parts\": [{\"text\": \"FICO: 620, Income: $4500. Do I qualify?\"}]}, {\"role\": \"model\", \"parts\": [{\"text\": \"Underwriting Decision: REJECTED. Reason: FICO score of 620 is below our 650 minimum.\"}]}]},\n",
        "    {\"contents\": [{\"role\": \"user\", \"parts\": [{\"text\": \"My FICO is 720 but I earn $2500/mo.\"}]}, {\"role\": \"model\", \"parts\": [{\"text\": \"Underwriting Decision: REJECTED. Reason: Monthly income of $2,500 is below our $3,000 requirement.\"}]}]},\n",
        "    {\"contents\": [{\"role\": \"user\", \"parts\": [{\"text\": \"FICO 680, Income $3500. Status?\"}]}, {\"role\": \"model\", \"parts\": [{\"text\": \"Underwriting Decision: APPROVED. You meet our minimum requirements (650 FICO / $3,000 income).\"}]}]},\n",
        "    {\"contents\": [{\"role\": \"user\", \"parts\": [{\"text\": \"Credit is 590, Income $10,000. Am I good?\"}]}, {\"role\": \"model\", \"parts\": [{\"text\": \"Underwriting Decision: REJECTED. Reason: FICO score 590 does not meet our 650 minimum.\"}]}]},\n",
        "    {\"contents\": [{\"role\": \"user\", \"parts\": [{\"text\": \"I earn $2950 and have an 800 FICO.\"}]}, {\"role\": \"model\", \"parts\": [{\"text\": \"Underwriting Decision: REJECTED. Reason: Income of $2,950 is below our $3,000 requirement.\"}]}]},\n",
        "    {\"contents\": [{\"role\": \"user\", \"parts\": [{\"text\": \"FICO 650, Income $3000. Is this enough?\"}]}, {\"role\": \"model\", \"parts\": [{\"text\": \"Underwriting Decision: APPROVED. You meet the exact minimum thresholds.\"}]}]},\n",
        "    {\"contents\": [{\"role\": \"user\", \"parts\": [{\"text\": \"My score is 750 and I make $12,000/mo.\"}]}, {\"role\": \"model\", \"parts\": [{\"text\": \"Underwriting Decision: APPROVED. Your profile significantly exceeds our minimum requirements.\"}]}]},\n",
        "    {\"contents\": [{\"role\": \"user\", \"parts\": [{\"text\": \"FICO is 649, Income $5000.\"}]}, {\"role\": \"model\", \"parts\": [{\"text\": \"Underwriting Decision: REJECTED. Reason: FICO score 649 is below our mandatory 650 minimum.\"}]}]},\n",
        "    {\"contents\": [{\"role\": \"user\", \"parts\": [{\"text\": \"High-earner at $15k/mo but my credit is 610.\"}]}, {\"role\": \"model\", \"parts\": [{\"text\": \"Underwriting Decision: REJECTED. Reason: FICO score of 610 is ineligible regardless of income.\"}]}]},\n",
        "    {\"contents\": [{\"role\": \"user\", \"parts\": [{\"text\": \"FICO: 710, Earnings: $3,200. Clear?\"}]}, {\"role\": \"model\", \"parts\": [{\"text\": \"Underwriting Decision: APPROVED. You meet Artha's eligibility criteria.\"}]}]}\n",
        "]\n",
        "\n",
        "# Write to train.jsonl\n",
        "with open('train.jsonl', 'w') as f:\n",
        "    for case in train_cases:\n",
        "        f.write(json.dumps(case) + '\\n')\n",
        "\n",
        "print(\"Created train.jsonl successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MnL1fgqHDlvu",
        "outputId": "f434550d-276b-4020-f80d-750a99d323b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created train.jsonl successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_cases = [\n",
        "    {\"contents\": [{\"role\": \"user\", \"parts\": [{\"text\": \"630 score and $6000 salary.\"}]}, {\"role\": \"model\", \"parts\": [{\"text\": \"Underwriting Decision: REJECTED. Reason: Minimum FICO required is 650. Your score is 630.\"}]}]},\n",
        "    {\"contents\": [{\"role\": \"user\", \"parts\": [{\"text\": \"Income: $2800, FICO: 750.\"}]}, {\"role\": \"model\", \"parts\": [{\"text\": \"Underwriting Decision: REJECTED. Reason: Minimum income required is $3,000. Your income is $2,800.\"}]}]},\n",
        "    {\"contents\": [{\"role\": \"user\", \"parts\": [{\"text\": \"Score 660, Income $3100.\"}]}, {\"role\": \"model\", \"parts\": [{\"text\": \"Underwriting Decision: APPROVED. You meet the minimum requirements.\"}]}]},\n",
        "    {\"contents\": [{\"role\": \"user\", \"parts\": [{\"text\": \"What if my FICO is 655 and income is $3050?\"}]}, {\"role\": \"model\", \"parts\": [{\"text\": \"Underwriting Decision: APPROVED. You exceed our minimum thresholds.\"}]}]},\n",
        "    {\"contents\": [{\"role\": \"user\", \"parts\": [{\"text\": \"Score 600, Income $2000.\"}]}, {\"role\": \"model\", \"parts\": [{\"text\": \"Underwriting Decision: REJECTED. Reason: Both FICO and income are below requirements.\"}]}]},\n",
        "    {\"contents\": [{\"role\": \"user\", \"parts\": [{\"text\": \"I have 700 FICO, make $2999.\"}]}, {\"role\": \"model\", \"parts\": [{\"text\": \"Underwriting Decision: REJECTED. Reason: Your income is $1 below our threshold.\"}]}]}, # Added comma here\n",
        "    {\"contents\": [{\"role\": \"user\", \"parts\": [{\"text\": \"Credit 651, Income $3001.\"}]}, {\"role\": \"model\", \"parts\": [{\"text\": \"Underwriting Decision: APPROVED. You meet our eligibility criteria.\"}]}]},\n",
        "    {\"contents\": [{\"role\": \"user\", \"parts\": [{\"text\": \"Loan with 640 FICO and $8000 income?\"}]}, {\"role\": \"model\", \"parts\": [{\"text\": \"Underwriting Decision: REJECTED. Reason: FICO score 640 is below the 650 requirement.\"}]}]},\n",
        "    {\"contents\": [{\"role\": \"user\", \"parts\": [{\"text\": \"Income $4200, FICO 675.\"}]}, {\"role\": \"model\", \"parts\": [{\"text\": \"Underwriting Decision: APPROVED. You meet our requirements.\"}]}]},\n",
        "    {\"contents\": [{\"role\": \"user\", \"parts\": [{\"text\": \"I earn $4166/mo and have a 700 score.\"}]}, {\"role\": \"model\", \"parts\": [{\"text\": \"Underwriting Decision: APPROVED. Both criteria qualify.\"}]}]}\n",
        "]\n",
        "\n",
        "# Write to validation.jsonl\n",
        "import json\n",
        "with open('validation.jsonl', 'w') as f:\n",
        "    for case in val_cases:\n",
        "        f.write(json.dumps(case) + '\\n')\n",
        "\n",
        "print(\"Created validation.jsonl successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-W0vyqpGpmS",
        "outputId": "c08251d7-1952-4e8b-b5a2-71357800111a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created validation.jsonl successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# This will trigger your browser to download the files to your 'Downloads' folder\n",
        "files.download('train.jsonl')\n",
        "files.download('validation.jsonl')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "jishB92McMbh",
        "outputId": "06dfaa8b-258a-43a5-93e2-1857831f1fac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_1b87a6bc-3782-462e-b006-11c456868ed9\", \"train.jsonl\", 2254)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_59c78a75-d5c2-4dbe-9131-584138e6b9dd\", \"validation.jsonl\", 2063)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import vertexai\n",
        "from vertexai.generative_models import GenerativeModel\n",
        "\n",
        "# 1. Configuration\n",
        "PROJECT_ID = \"soral-vertex-a\"\n",
        "LOCATION = \"us-central1\"\n",
        "# Get this ID from the Online Prediction > Endpoints page\n",
        "ENDPOINT_ID = \"527008017820942336\"\n",
        "\n",
        "# 2. Initialize Vertex AI\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
        "\n",
        "# 3. Reference the Shared Endpoint\n",
        "# Note the standard URI format: https://LOCATION-aiplatform.googleapis.com\n",
        "endpoint_path = f\"projects/{PROJECT_ID}/locations/{LOCATION}/endpoints/{ENDPOINT_ID}\"\n",
        "model = GenerativeModel(endpoint_path)\n",
        "\n",
        "# 4. Simple Chat Test\n",
        "def artha_chat(message):\n",
        "    try:\n",
        "        response = model.generate_content(message)\n",
        "        return response.text\n",
        "    except Exception as e:\n",
        "        return f\"Artha Connection Error: {str(e)}\"\n",
        "\n",
        "print(artha_chat(\"FICO 200, Income $5000. Am I eligible?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPrqY5B4iNvn",
        "outputId": "e2ed7cc9-f45c-4836-928a-667fe005e639"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Underwriting Decision: REJECTED. Reason: FICO score 200 is below our mandatory 650 minimum.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import vertexai\n",
        "from vertexai.generative_models import GenerativeModel, ChatSession\n",
        "\n",
        "# 1. Configuration\n",
        "PROJECT_ID = \"soral-vertex-a\"\n",
        "LOCATION = \"us-central1\"\n",
        "# Use your new Shared Endpoint ID here\n",
        "ENDPOINT_ID = \"527008017820942336\"\n",
        "\n",
        "# 2. Initialize Vertex AI\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
        "\n",
        "# 3. Reference the Shared Endpoint as a GenerativeModel\n",
        "endpoint_path = f\"projects/{PROJECT_ID}/locations/{LOCATION}/endpoints/{ENDPOINT_ID}\"\n",
        "model = GenerativeModel(endpoint_path)\n",
        "\n",
        "# 4. Define the Chat Function\n",
        "# message: the user's current input string\n",
        "# history: the list of previous turns (Gradio handles this for you)\n",
        "def predict(message, history):\n",
        "    # Start a chat session using the tuned model on the endpoint\n",
        "    chat = model.start_chat()\n",
        "\n",
        "    try:\n",
        "        # Send the user message to your tuned model\n",
        "        response = chat.send_message(message)\n",
        "        return response.text\n",
        "    except Exception as e:\n",
        "        return f\"‚ö†Ô∏è Artha Connection Error: {str(e)}\"\n",
        "\n",
        "# --- CHANGE THIS ---\n",
        "demo = gr.ChatInterface(\n",
        "    fn=predict,\n",
        "    title=\"Soral AI Underwriter\",\n",
        "    description=\"Custom-tuned model for loan eligibility gates.\",\n",
        "    # theme=\"soft\",  <-- REMOVE THIS LINE\n",
        "    examples=[\"FICO 720, Income $4500\", \"FICO 610, Income $8000\"]\n",
        ")\n",
        "\n",
        "# --- ADD IT HERE ---\n",
        "demo.launch(theme=\"soft\", share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "id": "n-2AvBbjnV6Y",
        "outputId": "0e68b279-e4df-4355-b3e9-75fe722815cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/vertexai/generative_models/_generative_models.py:433: UserWarning: This feature is deprecated as of June 24, 2025 and will be removed on June 24, 2026. For details, see https://cloud.google.com/vertex-ai/generative-ai/docs/deprecations/genai-vertexai-sdk.\n",
            "  warning_logs.show_deprecation_warning()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://e8df432441dbb405e3.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://e8df432441dbb405e3.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import vertexai\n",
        "from vertexai.generative_models import GenerativeModel, ChatSession, GenerationConfig\n",
        "from google.genai import types  # <--- This is where your configs live now\n",
        "\n",
        "# 1. Configuration\n",
        "PROJECT_ID = \"soral-vertex-a\"\n",
        "LOCATION = \"us-central1\"\n",
        "# Use your new Shared Endpoint ID here\n",
        "ENDPOINT_ID = \"3508390971140210688\"\n",
        "\n",
        "# 2. Initialize Vertex AI\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
        "\n",
        "# 2. Updated System Instructions\n",
        "SYSTEM_INSTRUCTIONS = \"\"\"\n",
        "# ROLE\n",
        "You are Soral AI, a playful yet professional Underwriting Assistant.\n",
        "\n",
        "\n",
        "# THE RULES OF THE GATE\n",
        "1. NAME: Ask for Full Name. Do not proceed until you have a string that looks like a name.\n",
        "2. ID: Ask for a 6-digit ID number.\n",
        "   - CRITICAL: If the user provides something that is NOT 6 digits, playfully tell them \"That doesn't look like a real ID!\" and ask for the ID AGAIN.\n",
        "   - DO NOT move to FICO until you have a valid 6-digit ID.\n",
        "3. FICO: Ask for FICO score (300-850).\n",
        "4. INCOME: Ask for monthly income.\n",
        "\n",
        "# ANTI-LOOP LOGIC\n",
        "- Never ask a question you have already successfully answered in the chat history.\n",
        "- If the user provides \"Wrong\" data for the current step, STAY on that step. Do not restart from Name.\n",
        "- Use the chat history to see which 'Gate' is currently open.\n",
        "\n",
        "# INTERNAL CHECKLIST (Do this before every reply)\n",
        "1. Do I have the Name? [ ]\n",
        "2. Do I have the ID? [ ]\n",
        "3. Do I have the FICO? [ ]\n",
        "4. Do I have the Income? [ ]\n",
        "\n",
        "Ask for the first item that is not checked. Once all are checked, provide the underwriting decision based on your policy.\n",
        "\"\"\"\n",
        "\n",
        "# 1. Define the Anti-Loop Configuration\n",
        "anti_loop_config = GenerationConfig(\n",
        "    temperature=0.1,       # Low but not 0; 0.1 allows slight variety to escape loops\n",
        "    presence_penalty=1.5,  # Encourages the model to talk about NEW topics\n",
        "    frequency_penalty=1.5, # Penalizes the model for repeating the EXACT same words\n",
        "    max_output_tokens=1024, # Prevents the model from talking forever\n",
        "    top_p=0.8              # Limits the word pool to the top 80% most likely words\n",
        ")\n",
        "\n",
        "# 3. Reference the Shared Endpoint as a GenerativeModel\n",
        "endpoint_path = f\"projects/{PROJECT_ID}/locations/{LOCATION}/endpoints/{ENDPOINT_ID}\"\n",
        "model = GenerativeModel(endpoint_path,\n",
        "                        system_instruction=[SYSTEM_INSTRUCTIONS],\n",
        "                        generation_config=anti_loop_config # Apply the settings here\n",
        "                        )\n",
        "\n",
        "\n",
        "# 4. Define the Chat Function\n",
        "# message: the user's current input string\n",
        "# history: the list of previous turns (Gradio handles this for you)\n",
        "def predict(message, history):\n",
        "    # Convert Gradio history format to Gemini history format\n",
        "    # This ensures the model 'remembers' it already has the name\n",
        "    chat = model.start_chat(history=history)\n",
        "    response = chat.send_message(message)\n",
        "    try:\n",
        "        # Send the user message to your tuned model\n",
        "        response = chat.send_message(message)\n",
        "        return response.text\n",
        "    except Exception as e:\n",
        "        return f\"‚ö†Ô∏è Artha Connection Error: {str(e)}\"\n",
        "\n",
        "# --- CHANGE THIS ---\n",
        "demo = gr.ChatInterface(\n",
        "    fn=predict,\n",
        "    title=\"Soral Underwriter AI\",\n",
        "    description=\"Custom-tuned model for loan eligibility gates.\",\n",
        "    # theme=\"soft\",  <-- REMOVE THIS LINE\n",
        "    examples=[\"FICO 720, Income $4500\", \"FICO 610, Income $8000\"]\n",
        ")\n",
        "\n",
        "# --- ADD IT HERE ---\n",
        "demo.launch(theme=\"soft\", share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "id": "BV84DkBnps-x",
        "outputId": "cd3ef90b-8322-4b6e-e974-17de7833c874"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/vertexai/generative_models/_generative_models.py:433: UserWarning: This feature is deprecated as of June 24, 2025 and will be removed on June 24, 2026. For details, see https://cloud.google.com/vertex-ai/generative-ai/docs/deprecations/genai-vertexai-sdk.\n",
            "  warning_logs.show_deprecation_warning()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://ae37ea4172006f9ef5.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://ae37ea4172006f9ef5.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    }
  ]
}